[
  {
    "title": "r/tucker_carlson celebrating Kenosha protest shooter Kyle Rittenhouse",
    "text": null
  },
  {
    "author": "joans34",
    "text": "Holy shit straight up anti-Semitic dog whistling\n\n[https://np.reddit.com/r/tucker\\_carlson/comments/ihboaz/his\\_name\\_is\\_kyle\\_rittenhouse/g2zcz8d/](https://np.reddit.com/r/tucker_carlson/comments/ihboaz/his_name_is_kyle_rittenhouse/g2zcz8d/)"
  },
  {
    "author": "[deleted]",
    "text": "[deleted]"
  },
  {
    "author": "Bardfinn",
    "text": "The author of that comment is an account I have flagged as an anti-Semitic bigot from postings in /r/The_Donald, 06/26/2019; The text of the first flag is lost from the archives I have (I guess I got to it before pushshift did and he nuked it before pushshift got to it) but the remainder of this person's comment / post history is the vilest racist garbage."
  },
  {
    "author": "KingSpartan15",
    "text": "And why won't Reddit perma ban them?"
  },
  {
    "author": "Bardfinn",
    "text": "Reddit needs user reports to action accounts / subreddits.\n\nThere's a **massive** social stigma against reporting.\n\nReddit has a model they've been using to help identify / characterise the problem of toxicity / hate speech on Reddit; Their figures are that **_only 8% of content their model characterised as likely toxic, was reported by users_**\n\nEvery person is limited to 10 reports per hour. In a 2 week period, they characterised what was posted, and found that 0.2% of the content posted to Reddit was \"potentially hateful\" -- yet, that 0.2% was _still_ **_~35,000 comments_**.\n\nIF 350 people _worked_ a \"full 8 hour day\", reporting 10 comments per hour, for 80 comments per day, they _still_ would not report all the potentially / modelled-as-toxic commentary on Reddit.\n\n30% of the potentially toxic material on Reddit is removed by moderators / automoderator rules.\n\nThat leaves ~24k items.\n\n#One of the things we want to do, at /r/AgainstHateSubreddits, is to teach every single person who reads our subreddit how to use the report options.\n\n##Reddit needs people to report hateful / harassing content in order to action it."
  },
  {
    "author": "KingSpartan15",
    "text": "Thanks for the insight and I agree.\n\nWhat I don't understand is why subs like r/actualpublicfreakouts aren't banned outright in their entirety.\n\nIt's literally a Nazi sub. Nuke the whole fucking thing.\n\nIf I was a mod it would take 2 damn seconds.\n\nI look at the sub.\n\nI see it's non stop racist and fascist\n\nI nuke the sub.\n\nWhy does this not happen?"
  },
  {
    "author": "Bardfinn",
    "text": "That said - I _have_ encountered a few times where I was going through a clearly-toxic subreddit, reporting items, and I right-click -> Open in a New Tab some clearly horrible post to look inside, and am met with a `This Community Has Been Banned ... Just Now` Subreddit Shutter Splash Screen ... and then an inbox notification that says that the admins investigated a report on an item in that subreddit and took action. Just ... clearly an admin found cause and shut it down in conjunction with my report."
  },
  {
    "author": "Bardfinn",
    "text": "Reddit is restricted by case law in the Ninth circuit - specifically _Mavrix Photographs LLC v LiveJournal Inc._ and the AOL Community Leader Program fallout.\n\nThe short and plain-English rundown of those two situations is this:\n\nIf a user-content-hosting ISP (like Reddit) pays employees to moderate content on their platform, then the ISP also can be liable for copyright violations on the platform that the moderator-employees fumble handling -- potentially -- because _Mavrix v LiveJournal_ isn't fully decided yet. Losing DMCA Safe Harbour is a potential result. That would bankrupt Reddit.\n\nSo Reddit -- in order to stay afloat _legally_, and avoid government regulators / lawsuits, remains \"agnostic\" of the content of subreddits.\n\nThey treat each report as an isolated incident, until and unless they have direct and incontrovertible proof directly from the moderators of a subreddit, that the subreddit violates -- by its nature or operation -- the Sitewide Rules and/or User Agreement ... they have to have a _case_ that will hold up in case they ever get sued by someone or investigated by some government regulator, and all their ducks in a row."
  },
  {
    "author": "CMDR_Expendible",
    "text": "Just as a matter of interest, is the difference between a volunteer moderator of a subreddit making a judgement call over time, and Reddit officially making such a call, that Reddit can't be held legally liable for the former, but only the latter?\n\nBecause a large part of the problem in dealing with online hatred is that you can't train an AI to recognise it well enough, and you need a human watcher, one that watches over time to handle the myriad ways other humans try and get around the rules.  But if you have those watchers in charge of the subreddits themselves, they'll never make the call to self ban their own hateful community. They'll only target each other, and then you risk being back to square one and Reddit needs to take a position it will judge upon, and defend.\n\nI know in my own case, a seriously unhinged stalker just kept swapping identities again and again and each time I reported it in to Reddit, I had to try and explain the backstory as to why I knew it was him all over again; I'm not sure that would even be possible now with the tiny report form Reddit allows you to fill in... The whole thing is frankly an unwholesome mess.\n\nBut good on the users here for at least trying to keep track of the hatred and continuing to flag it up."
  },
  {
    "author": "Bardfinn",
    "text": "Yep - there's lots of language in the User Agreement that seeks to disclaim liability for Reddit, Inc. from the things users and mods do -- and there's a reason the User Agreement has a section / clause to the effect of \"this user agreement contstitutes the entire agreement etc.\"\n\n>  if you have those watchers in charge of the subreddits themselves, they'll never make the call to self ban their own hateful community.\n\nThe age-old problem."
  },
  {
    "author": "IbnKafir",
    "text": ">\tIt’s literally a Nazi sub\n\nEveryone these days seems to forget words have meaning. What have you seen there that’s ‘openly Nazi’?"
  },
  {
    "author": "catgirl_apocalypse",
    "text": "Social stigma aside, reporting something to the admins and not the sub mods is incredibly cumbersome, especially on mobile."
  },
  {
    "author": "Bardfinn",
    "text": "Yep. Many people have given that feedback and we're told it's a high priority to overhaul the report flow - and that we should see results soon."
  },
  {
    "author": "[deleted]",
    "text": "[deleted]"
  },
  {
    "author": "Bardfinn",
    "text": "Those are really good suggestions! Thanks! \\^_^"
  },
  {
    "author": "MildlyAgreeable",
    "text": "Ok so this is off topic but, on dark mode, the red and blue colours on the letters make it look holographic. Tripped me right the fuck out.\n\nEdit [Solved.](https://en.m.wikipedia.org/wiki/Chromostereopsis)"
  },
  {
    "author": "MadGeekling",
    "text": "And a death threat... wow"
  },
  {
    "author": "pyroguy1104",
    "text": "C’mon, it’s just not a thread in /r/tucker_carlson without blatant antisemitic dogwhistling."
  },
  {
    "author": "towerator",
    "text": "It's less of a dogwhistle and more of a foghorn..."
  },
  {
    "author": "[deleted]",
    "text": "in the same comment section, people compared rioters to \"rabid animals\". so that's the type of people we're dealing with."
  },
  {
    "author": "-_Fiction_-",
    "text": "What’s new lol\n\nThis sorta stuff isn’t even tricky to find."
  }
]